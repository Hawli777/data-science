{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> 从安然公司邮件中发现欺诈证据-by hawli </h1> </center>\n",
    "<center> <h4> Udacity Data Analyst Nanodegree P5 </h4> </center>\n",
    "<h4> 数据集背景 </h4>\n",
    "安然电子邮件和财务数据集是关于安然公司的相关信息，安然是能源、商品和服务公司公司，由于惯性商业欺诈，2001年12月因破产而破产。在公司崩溃之后，联邦能源监管委员会在2000-2002年公布了安然公司高管们发送和接收的160万封电子邮件。经过多次关于电子邮件敏感性的投诉，FERC纠正了大部分电子邮件，但仍有大约50万人可供公众使用。电子邮件和财务数据包含电子邮件本身，例如从每个人收到的和发送的数字，以及包括薪资和股票期权的财务信息。\n",
    "\n",
    "安然数据集已经成为机器学习实践者的有价值的培训和测试工具，可以从数据中的特征中尝试和开发可以识别兴趣者（POI）的模型。感兴趣的人是最终在安然调查中被欺诈或犯罪活动的人，包括几名高层管理人员。该项目的目标是创建一个可以分离POI的机器学习模型。\n",
    "\n",
    "我使用了安然公司公布的原始数据。调查安然数据集的最终目标是能够提前预测欺诈或不安全的商业行为，有关人员可以受到惩罚，无辜人员可以避免伤害。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 异常值检测和数据清洗 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "#from tester import dump_classifier_and_data\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "df = df.replace('NaN', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据点总数:\t146\n",
      "POI数量:\t\t18\n",
      "非POI数量:\t128\n",
      "POI率:\t\t0.123287671233\n",
      "特征总数:\t21\n",
      "缺失值超过50%的特征: ['deferral_payments', 'loan_advances', 'restricted_stock_deferred', 'deferred_income', 'long_term_incentive', 'director_fees']\n",
      "POI中缺失值超过50%的特征: ['deferral_payments', 'loan_advances', 'restricted_stock_deferred', 'director_fees']\n"
     ]
    }
   ],
   "source": [
    "# 检查缺失值\n",
    "data_points = len(data_dict)\n",
    "poi_count = 0\n",
    "non_poi_count = 0\n",
    "missing_value_map = { 'bonus': {'count':0, 'poi':0}, 'deferral_payments': {'count':0, 'poi':0},\n",
    "    'deferred_income': {'count':0, 'poi':0},'director_fees': {'count':0, 'poi':0}, \n",
    "    'exercised_stock_options': {'count':0, 'poi':0}, 'total_payments': {'count':0, 'poi':0},\n",
    "    'expenses': {'count':0, 'poi':0}, 'loan_advances': {'count':0, 'poi':0},\n",
    "    'long_term_incentive': {'count':0, 'poi':0}, 'restricted_stock_deferred': {'count':0, 'poi':0},\n",
    "    'other': {'count':0, 'poi':0}, 'restricted_stock': {'count':0, 'poi':0}, \n",
    "    'total_stock_value': {'count':0, 'poi':0}, 'salary': {'count':0, 'poi':0}, \n",
    "    'email_address': {'count':0, 'poi':0}, 'from_messages': {'count':0, 'poi':0}, \n",
    "    'from_poi_to_this_person': {'count':0, 'poi':0}, 'shared_receipt_with_poi': {'count':0, 'poi':0},\n",
    "    'from_this_person_to_poi': {'count':0, 'poi':0}, 'to_messages': {'count':0, 'poi':0} }\n",
    "\n",
    "\n",
    "for person, features in data_dict.iteritems():    \n",
    "    isPoi = False    \n",
    "    if features['poi'] == True:\n",
    "        poi_count += 1\n",
    "        isPoi = True\n",
    "    else:\n",
    "        non_poi_count += 1\n",
    "    for name, value in features.iteritems():         \n",
    "        if value == 'NaN':\n",
    "            missing_value_map[name]['count'] += 1\n",
    "            if isPoi:\n",
    "                missing_value_map[name]['poi'] += 1\n",
    "\n",
    "print \"数据点总数:\\t\", data_points \n",
    "print \"POI数量:\\t\\t\", poi_count\n",
    "print \"非POI数量:\\t\", non_poi_count\n",
    "print \"POI率:\\t\\t\", poi_count/data_points\n",
    "print \"特征总数:\\t\", len(data_dict[data_dict.keys()[0]])\n",
    "\n",
    "\n",
    "significant_missing_values = []\n",
    "significant_poi_values = []\n",
    "\n",
    "#print \"{:<25} {:<20} {:<10}\".format('Feature','missing','poi')\n",
    "for feature, values in missing_value_map.iteritems():\n",
    "    missing_ratio = values['count']/data_points\n",
    "    if missing_ratio > 0.5:\n",
    "        significant_missing_values.append(feature)\n",
    "    poi_ratio = values['poi']/poi_count\n",
    "    if poi_ratio > 0.5:\n",
    "        significant_poi_values.append(feature)\n",
    "    #print \"{:<25} {:<20} {:<10}\".format(feature, values['count'], values['poi'])\n",
    "\n",
    "print \"缺失值超过50%的特征:\", significant_missing_values\n",
    "print \"POI中缺失值超过50%的特征:\", significant_poi_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调查的数据总量为146个，共21个特征，POI率为0.12，由于21个特征中有多个特征缺失值太多，因此选定余下几个特征进行数据探索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "财务特征数量:\t9\n",
      "邮件特征数量:\t5\n"
     ]
    }
   ],
   "source": [
    "# Explore dataset\n",
    "\n",
    "financial_features = ['salary', 'total_payments', 'bonus', \n",
    "                      'deferred_income', 'total_stock_value', 'expenses', \n",
    "                      'exercised_stock_options', 'other', 'restricted_stock']\n",
    "#'email_address',\n",
    "email_features = ['to_messages', 'from_poi_to_this_person', \n",
    "                  'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi']\n",
    "\n",
    "features_list = ['poi'] + financial_features + email_features\n",
    "df = df[features_list]\n",
    "print \"财务特征数量:\\t\", len(financial_features)\n",
    "print \"邮件特征数量:\\t\", len(email_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 去除财务表格上的异常值，这两行均非个人数据\n",
    "df.drop(axis=0, labels=['TOTAL','THE TRAVEL AGENCY IN THE PARK'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# 为财务特征缺失值填充为0 \n",
    "df[financial_features] = df[financial_features].fillna(0)\n",
    "\n",
    "# 为邮件特征填充均值\n",
    "imp = Imputer(missing_values='NaN', strategy = 'mean', axis=0)\n",
    "\n",
    "df_poi = df[df['poi'] == True];\n",
    "df_nonpoi = df[df['poi']==False]\n",
    "\n",
    "df_poi.loc[:, email_features] = imp.fit_transform(df_poi.loc[:,email_features]);\n",
    "df_nonpoi.loc[:, email_features] = imp.fit_transform(df_nonpoi.loc[:,email_features]);\n",
    "df = df_poi.append(df_nonpoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LAY KENNETH L         12\n",
       "FREVERT MARK A         9\n",
       "BELDEN TIMOTHY N       8\n",
       "SKILLING JEFFREY K     8\n",
       "LAVORATO JOHN J        7\n",
       "KEAN STEVEN J          7\n",
       "DELAINEY DAVID W       6\n",
       "WHALLEY LAWRENCE G     6\n",
       "KITCHEN LOUISE         6\n",
       "BAXTER JOHN C          6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查找是否有其他异常数据\n",
    "IQR = df.quantile(q=0.75) - df.quantile(q=0.25)\n",
    "first_quartile = df.quantile(q=0.25)\n",
    "third_quartile = df.quantile(q=0.75)\n",
    "outliers = df[(df>(third_quartile + 1.5*IQR) ) | (df<(first_quartile - 1.5*IQR) )].count(axis=1)\n",
    "outliers.sort_values(axis=0, ascending=False, inplace=True)\n",
    "outliers.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 剔除非poi的异常值\n",
    "df.drop(axis=0, labels=['FREVERT MARK A', 'LAVORATO JOHN J','KEAN STEVEN J', \n",
    "                        'WHALLEY LAWRENCE G', 'KITCHEN LOUISE','BAXTER JOHN C'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    120\n",
       "True      18\n",
       "Name: poi, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看剔除后的数据点\n",
    "len(df)\n",
    "df['poi'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 特征工程 </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步是从可能提高性能的现有信息中创建新功能。我还需要执行特征选择，以删除那些跟预测目标无关的特征。\n",
    "\n",
    "在考虑了安然案例的背景和数据集中包含的信息后，我决定从电子邮件原始数据创建三个新特征。第一个是电子邮件和个人的电子邮件与所有发送给该人的电子邮件的利益相关的比例，第二个是相同的邮件，但是对于poi的邮件的比例，第三个是共享的电子邮件与所有发给该人的电子邮件都有兴趣的人的比率。\n",
    "\n",
    "这些选择背后的理由是，考虑到个人发送或接收的电子邮件的总数，相对数量可能并不重要。我觉得与poi更多地交流的个人（如电子邮件所示）本身更有可能是poi，因为欺诈行为并不是单独个人形成，应该是一个社交网。当然，也会有部分无辜的人会收到poi的邮件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the new email features to the dataframe\n",
    "df['to_poi_ratio'] = df['from_poi_to_this_person'] / df['to_messages']\n",
    "df['from_poi_ratio'] = df['from_this_person_to_poi'] / df['from_messages']\n",
    "df['shared_poi_ratio'] = df['shared_receipt_with_poi'] / df['to_messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_list.append('to_poi_ratio')\n",
    "features_list.append('from_poi_ratio')\n",
    "features_list.append('shared_poi_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the new financial features and add to the dataframe\n",
    "df['bonus_to_salary'] = df['bonus'] / df['salary']\n",
    "df['bonus_to_total'] = df['bonus'] / df['total_payments'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_list.append('bonus_to_salary')\n",
    "features_list.append('bonus_to_total') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 特征缩放 </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征缩放可以使机器学习算法工作的更好。比如在K近邻算法中，分类器主要是计算两点之间的距离，如果一个特征比其它的特征有更大的范围值，那么距离将会被这个特征值所主导。因此每个特征应该被归一化，比如将取值范围处理为0到1之间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "# Fill any NaN financial data with a 0\n",
    "df.fillna(value= 0, inplace=True)\n",
    "\n",
    "# Create a copy of the dataframe and normalize it to zero mean and unit variance\n",
    "scaled_df = df.copy()\n",
    "scaled_df.iloc[:,1:] = scale(scaled_df.iloc[:,1:])\n",
    "\n",
    "\n",
    "# Send the dataset from dataframe to dictionary for tester.py\n",
    "my_dataset = scaled_df.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时，我还将使用财务数据创建新特征。我认为收到大额奖金的人可能更有可能成为感兴趣的人，因为奖金可能是欺诈活动的结果。将非法资金作为奖金更为轻松，而不是通常涉及合同和股东投入的加薪。这两个新功能将是与薪金相关的奖金，以及与总支付有关的奖金。现在共有19个特征，其中一些功能最有可能是减少或没有任何价值。我将执行功能缩减/选择来优化特征的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill any NaN financial data with a 0\n",
    "df.fillna(value= 0, inplace=True)\n",
    "\n",
    "# Create a copy of the dataframe and normalize it to zero mean and unit variance\n",
    "scaled_df = df.copy()\n",
    "scaled_df.iloc[:,1:] = scale(scaled_df.iloc[:,1:])\n",
    "\n",
    "# Send the dataset from dataframe to dictionary for tester.py\n",
    "my_dataset = scaled_df.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 四种算法对比 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.82421\tPrecision: 0.37520\tRecall: 0.34650\tF1: 0.36028\tF2: 0.35188\n",
      "\tTotal predictions: 14000\tTrue positives:  693\tFalse positives: 1154\tFalse negatives: 1307\tTrue negatives: 10846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import tester\n",
    "\n",
    "# Create the classifier, GaussianNB has no parameters to tune\n",
    "clf = GaussianNB()\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.89079\tPrecision: 0.62641\tRecall: 0.58350\tF1: 0.60419\tF2: 0.59160\n",
      "\tTotal predictions: 14000\tTrue positives: 1167\tFalse positives:  696\tFalse negatives:  833\tTrue negatives: 11304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.86043\tPrecision: 0.51722\tRecall: 0.34550\tF1: 0.41427\tF2: 0.37007\n",
      "\tTotal predictions: 14000\tTrue positives:  691\tFalse positives:  645\tFalse negatives: 1309\tTrue negatives: 11355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=None, tol=0.0001, verbose=0)\n",
      "\tAccuracy: 0.77236\tPrecision: 0.16526\tRecall: 0.14650\tF1: 0.15531\tF2: 0.14990\n",
      "\tTotal predictions: 14000\tTrue positives:  293\tFalse positives: 1480\tFalse negatives: 1707\tTrue negatives: 10520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = KMeans(n_clusters=2)\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在添加特征之后，所有算法的结果总结如下：\n",
    "\n",
    "| Classifier            | Precision | Recall  | F1 Score | Accuracy |\n",
    "|-----------------------|-----------|---------|----------|----------|\n",
    "| GaussianNB            | 0.37520   | 0.34650 | 0.36028  | 0.82421  |\n",
    "| DecisionTree          | 0.62641   | 0.58350 | 0.60419  | 0.89079  |\n",
    "| SVC (kernel='linear') | 0.51722   | 0.34550 | 0.41427  | 0.86043  |\n",
    "| KMeans (n_clusters=2) | 0.16526   | 0.14650 | 0.15531  | 0.77236  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树的F1最高，其次是支持向量机，在这一点上，k均值不能通过精确度和召回率都满足0.3的需求，因此舍弃该算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 特征选择 </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征选择是一个重要的数据预处理过程，进行特征选择有两个方面的原因，首先，我们在现实任务重经常会遇到维数灾难问题，这是由于属性过多造成的，其次，去除不相关的特征往往会降低学习任务的难度，留下关键因素。特征选择有三种方法，分别为过滤型，包裹型和内嵌型，我们先看一下决策树和adaboost分类起的功能重要性排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.89036\tPrecision: 0.62480\tRecall: 0.58200\tF1: 0.60264\tF2: 0.59008\n",
      "\tTotal predictions: 14000\tTrue positives: 1164\tFalse positives:  699\tFalse negatives:  836\tTrue negatives: 11301\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_tree = DecisionTreeClassifier()\n",
    "tester.test_classifier(clf_tree, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Feature Importances:\n",
      "\n",
      "from_poi_ratio : 0.3897\n",
      "expenses : 0.2446\n",
      "shared_receipt_with_poi : 0.1571\n",
      "to_poi_ratio : 0.0777\n",
      "shared_poi_ratio : 0.0598\n",
      "restricted_stock : 0.0359\n",
      "from_poi_to_this_person : 0.0351\n",
      "salary : 0.0000\n",
      "total_payments : 0.0000\n",
      "bonus : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Get the feature importances of the DecisionTree Classifier\n",
    "tree_feature_importances = clf_tree.feature_importances_\n",
    "tree_features = zip(tree_feature_importances, features_list[1:])\n",
    "tree_features = sorted(tree_features, key= lambda x:x[0], reverse=True)\n",
    "\n",
    "# Display the feature names and importance values\n",
    "print('Tree Feature Importances:\\n')\n",
    "for i in range(10):\n",
    "    print('{} : {:.4f}'.format(tree_features[i][1], tree_features[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.91964\tPrecision: 0.76245\tRecall: 0.63550\tF1: 0.69321\tF2: 0.65739\n",
      "\tTotal predictions: 14000\tTrue positives: 1271\tFalse positives:  396\tFalse negatives:  729\tTrue negatives: 11604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_ada = AdaBoostClassifier()\n",
    "tester.test_classifier(clf_ada, my_dataset, features_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada Boost Feature Importances:\n",
      "\n",
      "from_poi_ratio : 0.1600\n",
      "shared_receipt_with_poi : 0.1400\n",
      "to_poi_ratio : 0.1000\n",
      "shared_poi_ratio : 0.1000\n",
      "expenses : 0.0800\n",
      "to_messages : 0.0800\n",
      "deferred_income : 0.0600\n",
      "total_stock_value : 0.0600\n",
      "other : 0.0400\n",
      "from_this_person_to_poi : 0.0400\n"
     ]
    }
   ],
   "source": [
    "# Get the feature importances for the AdaBoost Classifier\n",
    "ada_feature_importances = clf_ada.feature_importances_\n",
    "ada_features = zip(ada_feature_importances, features_list[1:])\n",
    "\n",
    "# Display the feature names and importance values\n",
    "print('Ada Boost Feature Importances:\\n')\n",
    "ada_features = sorted(ada_features, key=lambda x:x[0], reverse=True)\n",
    "for i in range(10):\n",
    "    print('{} : {:.4f}'.format(ada_features[i][1], ada_features[i][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从决策树和adaboost的特征重要性上来看，具有不同的结果，接下来我们用SelectKBest并具有自动选择用于分类器的由方差量定义的k最佳特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "data_dict = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "n_features = np.arange(1, len(features_list))\n",
    "\n",
    "# Create a pipeline with feature selection and classification\n",
    "pipe = Pipeline([\n",
    "    ('select_features', SelectKBest()),\n",
    "    ('classify', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'select_features__k': n_features\n",
    "    }\n",
    "]\n",
    "\n",
    "# Use GridSearchCV to automate the process of finding the optimal number of features\n",
    "tree_clf= GridSearchCV(pipe, param_grid=param_grid, cv = 10)\n",
    "tree_clf.fit(features, labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'select_features__k': 19}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Feature F-statistics:\n",
      "\n",
      "bonus : 39.4293\n",
      "total_stock_value : 26.6422\n",
      "exercised_stock_options : 26.4094\n",
      "salary : 26.3390\n",
      "from_poi_ratio : 25.3142\n",
      "bonus_to_total : 21.3214\n",
      "shared_receipt_with_poi : 19.2004\n",
      "bonus_to_salary : 19.1163\n",
      "deferred_income : 16.7689\n",
      "from_poi_to_this_person : 16.3089\n",
      "shared_poi_ratio : 15.6486\n",
      "restricted_stock : 10.9824\n",
      "total_payments : 9.5521\n",
      "other : 7.7226\n",
      "expenses : 6.2723\n",
      "from_this_person_to_poi : 5.4742\n",
      "to_poi_ratio : 2.7360\n",
      "to_messages : 2.4327\n",
      "from_messages : 0.7822\n"
     ]
    }
   ],
   "source": [
    "tree_selection = SelectKBest(k=19)\n",
    "tree_selection.fit_transform(features, labels)\n",
    "\n",
    "tree_scores = tree_selection.scores_\n",
    "tree_features = zip(tree_scores, features_list[1:])\n",
    "tree_features = sorted(tree_features, key= lambda x:x[0], reverse=True)\n",
    "\n",
    "print('Tree Feature F-statistics:\\n')\n",
    "for i in range(19):\n",
    "    print('{} : {:.4f}'.format(tree_features[i][1], tree_features[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('select_features', SelectKBest(k=19, score_func=<function f_classif at 0x10e858938>)), ('classify', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.89329\tPrecision: 0.63558\tRecall: 0.59300\tF1: 0.61355\tF2: 0.60105\n",
      "\tTotal predictions: 14000\tTrue positives: 1186\tFalse positives:  680\tFalse negatives:  814\tTrue negatives: 11320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_clf = Pipeline([\n",
    "    ('select_features', SelectKBest(k=19)),\n",
    "    ('classify', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "tester.dump_classifier_and_data(tree_clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于最佳的特征选择k值为19，因此保持所有的特征进行后续分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 选择和调整算法 </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调整第一个算法的参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('select_features', SelectKBest(k=19, score_func=<function f_classif at 0x10e858938>)), ('classify', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'classify__max_features': [None, 'sqrt', 'log2', 'auto'], 'classify__min_samples_split': [2, 4, 6, 8, 10, 20], 'classify__criterion': ['gini', 'entropy'], 'classify__max_depth': [None, 5, 10, 15, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline with feature selection and classifier\n",
    "tree_pipe = Pipeline([\n",
    "    ('select_features', SelectKBest(k=19)),\n",
    "    ('classify', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "# Define the configuration of parameters to test with the \n",
    "# Decision Tree Classifier\n",
    "param_grid = dict(classify__criterion = ['gini', 'entropy'] , \n",
    "                  classify__min_samples_split = [2, 4, 6, 8, 10, 20],\n",
    "                  classify__max_depth = [None, 5, 10, 15, 20],\n",
    "                  classify__max_features = [None, 'sqrt', 'log2', 'auto'])\n",
    "\n",
    "# Use GridSearchCV to find the optimal hyperparameters for the classifier\n",
    "tree_clf = GridSearchCV(tree_pipe, param_grid = param_grid, cv=10)\n",
    "tree_clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classify__criterion': 'entropy',\n",
       " 'classify__max_depth': None,\n",
       " 'classify__max_features': None,\n",
       " 'classify__min_samples_split': 20}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the best algorithm hyperparameters for the Decision Tree\n",
    "tree_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('select_features', SelectKBest(k=19, score_func=<function f_classif at 0x10e858938>)), ('classify', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.92036\tPrecision: 0.72048\tRecall: 0.72300\tF1: 0.72174\tF2: 0.72249\n",
      "\tTotal predictions: 14000\tTrue positives: 1446\tFalse positives:  561\tFalse negatives:  554\tTrue negatives: 11439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the classifier with the optimal hyperparameters as found by GridSearchCV\n",
    "tree_clf = Pipeline([\n",
    "    ('select_features', SelectKBest(k=19)),\n",
    "    ('classify', DecisionTreeClassifier(criterion='entropy', max_depth=None, max_features=None, min_samples_split=20))\n",
    "])\n",
    "\n",
    "# Test the classifier using tester.py\n",
    "tester.dump_classifier_and_data(tree_clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调整第二个算法的参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('select_features', SelectKBest(k=19, score_func=<function f_classif at 0x10e858938>)), ('classify', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'classify__learning_rate': [0.5, 1, 1.5, 2, 4], 'classify__n_estimators': [30, 50, 70, 120], 'classify__base_estimator': [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, ..._score=False, random_state=None,\n",
       "            verbose=0, warm_start=False), GaussianNB(priors=None)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "# Create the pipeline with feature selection and AdaBoostClassifier\n",
    "ada_pipe = Pipeline([('select_features', SelectKBest(k=19)),\n",
    "                     ('classify', AdaBoostClassifier())\n",
    "                    ])\n",
    "\n",
    "# Define the parameter configurations to test with GridSearchCV\n",
    "param_grid = dict(classify__base_estimator=[DecisionTreeClassifier(), RandomForestClassifier(), GaussianNB()],\n",
    "                  classify__n_estimators = [30, 50, 70, 120],\n",
    "                  classify__learning_rate = [0.5, 1, 1.5, 2, 4])\n",
    "\n",
    "# Use GridSearchCV to automate the process of finding the optimal parameters\n",
    "ada_clf = GridSearchCV(ada_pipe, param_grid=param_grid, cv=10)\n",
    "ada_clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classify__base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best'),\n",
       " 'classify__learning_rate': 4,\n",
       " 'classify__n_estimators': 70}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the best parameters for the AdaBoostClassifier\n",
    "ada_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('select_features', SelectKBest(k=19, score_func=<function f_classif at 0x10e858938>)), ('classify', AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,...andom_state=None, splitter='best'),\n",
      "          learning_rate=4, n_estimators=70, random_state=None))])\n",
      "\tAccuracy: 0.88771\tPrecision: 0.61432\tRecall: 0.57500\tF1: 0.59401\tF2: 0.58246\n",
      "\tTotal predictions: 14000\tTrue positives: 1150\tFalse positives:  722\tFalse negatives:  850\tTrue negatives: 11278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implement the AdaBoost Classifier with the optimal parameters\n",
    "ada_clf = Pipeline([('select_features', SelectKBest(k=19)),\n",
    "                   ('classify', AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), learning_rate=4, n_estimators=70))\n",
    "                   ])\n",
    "\n",
    "# Test the classifier with cross-validation\n",
    "tester.dump_classifier_and_data(ada_clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行算法的最终版本的结果如下所示：\n",
    "\n",
    "\n",
    "|           算法          | 精确度    | 召回率   | F1 Score | 准确性    |\n",
    "|------------------------|-----------|--------|----------|----------|\n",
    "| DecisionTreeClassifier | 0.720     | 0.723  | 0.721    | 0.920    |\n",
    "| AdaBoostClassifier     | 0.614     | 0.575  | 0.594    | 0.888    |\n",
    "\n",
    "\n",
    "基于这些结果，我选择的最终算法是DecisionTreeClassifier。我的最终模型由决策树算法，SelectKBest特征选择的19个特征以及上面定义的模型超参数组成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 验证和评估 </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一步是要对算法进行交叉验证，因为要确认算法是否有效，部分训练集上呈现的有效性，不一定能够在全局有效。在用强大的算法寻找模型的模式时，存在过拟合的风险，也就是训练数据集上得到的结论无法推广到一般情况。因此，评价模型好坏可以用交叉验证的方式。主要可以从以下两个参数度量：\n",
    "\n",
    "精确度（precision） 顾名思义就是我找的准不准，我找出来的嫌疑人中，有多少是真正的嫌疑人，高的precision，会容易漏掉一些Poi,但不容易误伤好人。\n",
    "\n",
    "召回率（recall），顾名思义就是反正我的数据查的全不全，放在安然这个案例里，就是我找到的POI全不全。所以，高recall能够做到“宁可误杀好人，也要找到所有嫌疑人”。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('select_features', SelectKBest(k=19, score_func=<function f_classif at 0x10e858938>)), ('classify', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=20,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=4, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.90493\tPrecision: 0.67783\tRecall: 0.63750\tF1: 0.65705\tF2: 0.64518\n",
      "\tTotal predictions: 14000\tTrue positives: 1275\tFalse positives:  606\tFalse negatives:  725\tTrue negatives: 11394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tester.dump_classifier_and_data(tree_clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们测试分类器来进行交叉验证，分别从精确度、召回率和F1分数进行了记录。在tester.py中，我看到交叉验证产生k个大小相同的互斥子集，随机种子为了产生可重复的结果，交叉验证参数设置为42。我改变了随机的种子数的时候，模型的表现也降低了。因此，为了防止在交叉验证中随机种子在训练集上做出过度拟合的经典错误，这是需要注意的问题。即使采取预防措施来防止过拟合，我仍然针对一组特定的数据优化了我的模型。为了更好地指出决策树模型的性能指标，我用不同的随机种子进行了10次测试，发现了平均绩效指标。我的模型的最终结果总结如下：\n",
    "\n",
    "|           算法          | 精确度    | 召回率   | F1 Score | 准确性    |\n",
    "|------------------------|-----------|--------|----------|----------|\n",
    "| DecisionTreeClassifier | 0.678     | 0.638  | 0.657    | 0.904    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 结论 </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "精确度得分为0.678，表明我的模型有67.8%的人被标注为poi，召回率为0.638，表明我的模型里面有63.8%的人是可以确定为poi。\n",
    "\n",
    "在调整算法的过程中，我认为与其通过不同方式对算法的进行少量调整，也只让F1的分数提高了0.05，不如对提高原始数据的质量。在未来的机器学习模型的开发过程中，我将会集中精力收集尽可能多的高质量数据，再可以考虑训练和调整算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考资料\n",
    "\n",
    "https://github.com/dshgna/ud120-projects/blob/44305deefe627373eb2dcb5e0a1fb22026516458/final_project/poi_id.py\n",
    "\n",
    "https://github.com/WillKoehrsen/ud120-projects/blob/master/final_project/Enron_Machine_Learning_with_Python.pdf\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "http://blog.csdn.net/xlinsist/article/details/51212348\n",
    "\n",
    "http://www.cnblogs.com/jasonfreak/p/5448385.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
